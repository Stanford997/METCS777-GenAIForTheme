{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# to load and transform image datasets to tensor\n",
    "from datasets import load_dataset, load_from_disk, VerificationMode\n",
    "from datasets.arrow_dataset import Dataset\n",
    "# diffusers model\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler, StableDiffusionPipeline\n",
    "from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
    "\n",
    "# Text processing\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "# Image processing\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.transforms import InterpolationMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from configs import DATA_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from accelerate.utils import write_basic_config\n",
    "\n",
    "write_basic_config()  # Write a config file\n",
    "os._exit(00)  # Restart the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Note that step 1, 2, 3 should be migrated to EMR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Images\n",
    "Using the imagefolder feature of HuggingFace's load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_dataset: Dataset = load_dataset(\"imagefolder\", data_dir=DATA_FOLDER, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_dataset[0].get(\"image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Process Image\n",
    "\n",
    "(References: \n",
    "https://huggingface.co/docs/diffusers/v0.27.2/en/training/text2image  \n",
    "https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image.py#L40  \n",
    "https://pytorch.org/vision/main/transforms.html  \n",
    ")\n",
    "\n",
    "This step should be moved to EMR, which includes\n",
    "+ Resize Image to 512x512 (with Bilinear interpolation) (For stable diffusion 512 seems to be the optimal size, but for some images it might distort the images, so we might want to consider just add padding?)\n",
    "+ normalize images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2\n",
    "from torchvision.transforms import InterpolationMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform_pipeline = v2.Compose([\n",
    "    # TODO\n",
    "    # Instead of resize, enlarge the photo by ratio and add padding\n",
    "    v2.Resize(size=(512, 512), interpolation=InterpolationMode.LANCZOS),\n",
    "    v2.ToTensor(),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "new_img = transform_pipeline(img_dataset[0].get(\"image\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Process Caption\n",
    "To process caption (in english), we need a Cliptextmodel\n",
    "+ https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel  \n",
    "+ https://huggingface.co/runwayml/stable-diffusion-v1-5  \n",
    "+ https://huggingface.co/openai/clip-vit-large-patch14  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_train(examples):\n",
    "    # examples are a batch of 4 images\n",
    "    # we apply the transformation (reference above for what it transfomed to)\n",
    "    # then apply the tokenization\n",
    "\n",
    "    examples[\"pixel_values\"] = [transform_pipeline(image) for image in examples[\"image\"]]\n",
    "    \n",
    "    inputs = tokenizer([example for example in examples[\"caption\"]],\n",
    "                       padding=\"max_length\",\n",
    "                       truncation=True,\n",
    "                       return_tensors=\"pt\")\n",
    "\n",
    "    examples[\"input_ids\"] = inputs.input_ids\n",
    "    return examples\n",
    "\n",
    "train_set = img_dataset.with_transform(preprocess_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Upload to S3 parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note that Step 4 and beyond is carried out in SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Setup CLIP Embedding, VAE, UNET and remaining part of architecture for training\n",
    "other args we can try, like whether to use EMA or not https://huggingface.co/stabilityai/stable-diffusion-2-1/discussions/22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "type(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5. Get a scheduler for adding noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training loop\n",
    "This train loop is referenced from Huggingface training script https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image.py#L912\n",
    "\n",
    "Ref: https://huggingface.co/docs/accelerate/quicktour  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: add checkpoint and resume from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    \"\"\"\n",
    "    Collate Function is used to create a batch\n",
    "    \"\"\"\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "    input_ids = torch.stack([example[\"input_ids\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"input_ids\": input_ids}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "A bit research on Adding noise step:\n",
    "+ input_perturbation\n",
    "+ noise_offset\n",
    "\n",
    "Prediction Type: epsilon vs v_prediction \n",
    "https://medium.com/@zljdanceholic/three-stable-diffusion-training-losses-x0-epsilon-and-v-prediction-126de920eb73\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import LMSDiscreteScheduler\n",
    "\n",
    "def train():\n",
    "    # Getting the model weights from the pretrained models hub\n",
    "    \n",
    "    text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "    vae = AutoencoderKL.from_pretrained(\n",
    "        \"runwayml/stable-diffusion-v1-5\", subfolder=\"vae\"\n",
    "    )\n",
    "\n",
    "    unet = UNet2DConditionModel.from_pretrained(\n",
    "        \"runwayml/stable-diffusion-v1-5\", subfolder=\"unet\"\n",
    "    )\n",
    "\n",
    "    # Freeze vae and text_encoder (we only train the UNET)\n",
    "\n",
    "    for params in vae.parameters():\n",
    "        params.requires_grad = False\n",
    "\n",
    "    for params in text_encoder.parameters():\n",
    "        params.requires_grad = False\n",
    "        \n",
    "    noise_scheduler = PNDMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "\n",
    "    lr = 0.001\n",
    "    batch_size = 4\n",
    "    optimizer = torch.optim.Adam(unet.parameters(), lr=lr)\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
    "                                              shuffle=True, collate_fn=collate_fn, num_workers=1)\n",
    "    accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "    device = accelerator.device\n",
    "\n",
    "    epochs = 2\n",
    "    weight_dtype = torch.float32\n",
    "\n",
    "    unet, optimizer, train_loader, noise_scheduler = accelerator.prepare(\n",
    "        unet, optimizer, train_loader, noise_scheduler\n",
    "    )\n",
    "\n",
    "    prediction_type = \"v_prediction\"\n",
    "\n",
    "    # Move vae and unet to device\n",
    "    vae.to(accelerator.device)\n",
    "    text_encoder.to(accelerator.device)\n",
    "\n",
    "    accelerator.wait_for_everyone()\n",
    "    for epoch in range(epochs):\n",
    "        text_encoder.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            with accelerator.accumulate(unet):\n",
    "                # First encode the image to laten space with the VAE encoder\n",
    "                latent = vae.encode(batch[\"pixel_values\"].to(weight_dtype)).latent_dist.sample()\n",
    "\n",
    "                # sample noise to add to latent\n",
    "                noise = torch.randn_like(latent)\n",
    "                bsz = latent.shape[0]\n",
    "\n",
    "                # Sample a random timestep for each image\n",
    "                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latent.device)\n",
    "                timesteps = timesteps.long()\n",
    "\n",
    "                # Add noise to the latents according to the noise magnitude at each timestep\n",
    "                # (this is the forward diffusion process)\n",
    "                noisy_latents = noise_scheduler.add_noise(latent, noise, timesteps)\n",
    "\n",
    "                # Get the text embedding for conditioning\n",
    "                encoder_hidden_states = text_encoder(batch[\"input_ids\"], return_dict=False)[0]\n",
    "\n",
    "                # Get the target for loss depending on the prediction type\n",
    "                if prediction_type is not None:\n",
    "                    # set prediction_type of scheduler if defined\n",
    "                    noise_scheduler.register_to_config(prediction_type=prediction_type)\n",
    "\n",
    "                if noise_scheduler.config.prediction_type == \"epsilon\":\n",
    "                    target = noise\n",
    "                elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
    "                    target = noise_scheduler.get_velocity(latent, noise, timesteps)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n",
    "\n",
    "                # Predict the noise residual and compute loss\n",
    "                model_pred = unet(noisy_latents, timesteps, encoder_hidden_states, return_dict=False)[0]\n",
    "\n",
    "                loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
    "\n",
    "                # Backpropagate\n",
    "                accelerator.backward(loss)\n",
    "                optimizer.step()\n",
    "                noise_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            if accelerator.sync_gradients:\n",
    "                accelerator.clip_grad_norm_(unet.parameters(), 1)\n",
    "        accelerator.wait_for_everyone()\n",
    "\n",
    "    # Create the pipeline using the trained modules and save it.\n",
    "    if accelerator.is_main_process:\n",
    "        pipeline = StableDiffusionPipeline(\n",
    "            text_encoder=accelerator.unwrap_model(text_encoder),\n",
    "            vae=vae,\n",
    "            unet=unet.module if accelerator.num_processes >1 else unet,\n",
    "            tokenizer=tokenizer,\n",
    "            scheduler=PNDMScheduler(\n",
    "                beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", skip_prk_steps=True\n",
    "            ),\n",
    "            safety_checker=safety_checker,\n",
    "            feature_extractor=feature_extractor,\n",
    "        )\n",
    "        pipeline.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "    accelerator.end_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from accelerate import notebook_launcher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "notebook_launcher(train, num_processes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
